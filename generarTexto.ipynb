{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generarTexto.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cwn00-wJdlS8",
        "outputId": "43b84528-6256-4391-d829-7e25959422bd"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o7kZ16dS65X"
      },
      "source": [
        "import numpy\n",
        "import sys\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAMODn5HTE3I"
      },
      "source": [
        "file = open(\"travel.txt\").read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQDfS6b8THpU",
        "outputId": "f50d64cf-7d88-4b25-e450-0a6601fb8afe"
      },
      "source": [
        "print('Length of text: {} characters'.format(len(file)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 5684 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKYeqMn3TLoM",
        "outputId": "b2aa9efa-21fe-4a2d-99bb-cd069027c733"
      },
      "source": [
        "print(file[:350])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traveling is easier than you think.\n",
            "We believe that traveling around the world shouldn’t be hard: it’s actually something everyone should be able to do at least once in their lives. Whether you choose to spend a few years or just a couple months traveling this beautiful planet, it’s important to see what’s out there. It’s up to you to make the drea\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVxOeercTQyT"
      },
      "source": [
        "def tokenize_words(input):\n",
        "    # El texto se pasa todo a minuscula , se estandariza\n",
        "    input = input.lower()\n",
        "\n",
        "    # se instancia el tokenizador\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(input)\n",
        "\n",
        "    # si el token creado no está en las palabras vacías, conviértalo en parte de \"filtrado\"\n",
        "    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
        "    return \" \".join(filtered)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3He7cENgTS1I"
      },
      "source": [
        "processed_inputs = tokenize_words(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY3YIC42TU0x"
      },
      "source": [
        "#los caracteres pasan a ser numeros asi que cada numero representaria los caracteres\n",
        "chars = sorted(list(set(processed_inputs)))\n",
        "char_to_num = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bD_JUfOd9X3",
        "outputId": "d113aba6-2bf4-4643-e2f9-f3f65de5033b"
      },
      "source": [
        "input_len = len(processed_inputs)\n",
        "vocab_len = len(chars)\n",
        "print (\"Total number of characters:\", input_len)\n",
        "print (\"Total vocab:\", vocab_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of characters: 3594\n",
            "Total vocab: 27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4WIxRpDd_Xk"
      },
      "source": [
        "#se hacen conjuntos de datos y se empieza a alimentar la red \n",
        "seq_length = 450\n",
        "x_data = []\n",
        "y_data = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTsaM9f3eBdN"
      },
      "source": [
        "#se revisa la lista de las entradas y se convierte a numero , esto hara que se crean secuencias \n",
        "# cada secuencia comienza con el siguiente caracter\n",
        "for i in range(0, input_len - seq_length, 1):\n",
        "    # Definir secuencias de entrada y salida\n",
        "     # La entrada es el carácter actual más la longitud de secuencia deseada\n",
        "    in_seq = processed_inputs[i:i + seq_length]\n",
        "\n",
        "    # La secuencia de salida es el carácter inicial más la longitud total de la secuencia\n",
        "    out_seq = processed_inputs[i + seq_length]\n",
        "\n",
        "\n",
        "    # Ahora convertimos la lista de caracteres a números enteros según\n",
        "     # previamente y agregue los valores a nuestras listas\n",
        "    x_data.append([char_to_num[char] for char in in_seq])\n",
        "    y_data.append(char_to_num[out_seq])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1Q0pGvieDV_",
        "outputId": "d1f90a5b-726c-4b1b-c140-00df37c00e70"
      },
      "source": [
        "#numero de secuencias de entrada\n",
        "n_patterns = len(x_data)\n",
        "print (\"Total Patterns:\", n_patterns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns: 3144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uyjlv7DReE-e"
      },
      "source": [
        "X = numpy.reshape(x_data, (n_patterns, seq_length, 1))\n",
        "X = X/float(vocab_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JchKssa5eHW5"
      },
      "source": [
        "#se codifica los datos de etiqueta\n",
        "y = np_utils.to_categorical(y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raIhc9j4eJuo"
      },
      "source": [
        "#la capa dense es la que generara la probabilidad de cual va ser la siguiente palabra\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3jDqNG4eL1D"
      },
      "source": [
        "#se compila el modelo\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yRGlaEveOPT"
      },
      "source": [
        "filepath = \"model_weights_saved.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "desired_callbacks = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXa8_UhceQpZ",
        "outputId": "a6ddd804-4de1-445b-9f1d-edc38a3c2cbc"
      },
      "source": [
        "model.fit(X, y, epochs=40, batch_size=256, callbacks=desired_callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 3.0468 \n",
            "Epoch 00001: loss improved from inf to 3.04685, saving model to model_weights_saved.hdf5\n",
            "13/13 [==============================] - 197s 15s/step - loss: 3.0468\n",
            "Epoch 2/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9503 \n",
            "Epoch 00002: loss improved from 3.04685 to 2.95032, saving model to model_weights_saved.hdf5\n",
            "13/13 [==============================] - 197s 15s/step - loss: 2.9503\n",
            "Epoch 3/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9320 \n",
            "Epoch 00003: loss improved from 2.95032 to 2.93202, saving model to model_weights_saved.hdf5\n",
            "13/13 [==============================] - 194s 15s/step - loss: 2.9320\n",
            "Epoch 4/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9313 \n",
            "Epoch 00004: loss improved from 2.93202 to 2.93133, saving model to model_weights_saved.hdf5\n",
            "13/13 [==============================] - 193s 15s/step - loss: 2.9313\n",
            "Epoch 5/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9273 \n",
            "Epoch 00005: loss improved from 2.93133 to 2.92727, saving model to model_weights_saved.hdf5\n",
            "13/13 [==============================] - 194s 15s/step - loss: 2.9273\n",
            "Epoch 6/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9226 \n",
            "Epoch 00006: loss improved from 2.92727 to 2.92264, saving model to model_weights_saved.hdf5\n",
            "13/13 [==============================] - 193s 15s/step - loss: 2.9226\n",
            "Epoch 7/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9276 \n",
            "Epoch 00007: loss did not improve from 2.92264\n",
            "13/13 [==============================] - 192s 15s/step - loss: 2.9276\n",
            "Epoch 8/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9217 \n",
            "Epoch 00008: loss improved from 2.92264 to 2.92172, saving model to model_weights_saved.hdf5\n",
            "13/13 [==============================] - 195s 15s/step - loss: 2.9217\n",
            "Epoch 9/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9191 \n",
            "Epoch 00009: loss improved from 2.92172 to 2.91905, saving model to model_weights_saved.hdf5\n",
            "13/13 [==============================] - 194s 15s/step - loss: 2.9191\n",
            "Epoch 10/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9316 \n",
            "Epoch 00010: loss did not improve from 2.91905\n",
            "13/13 [==============================] - 193s 15s/step - loss: 2.9316\n",
            "Epoch 11/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9194 \n",
            "Epoch 00011: loss did not improve from 2.91905\n",
            "13/13 [==============================] - 195s 15s/step - loss: 2.9194\n",
            "Epoch 12/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9147 \n",
            "Epoch 00012: loss improved from 2.91905 to 2.91475, saving model to model_weights_saved.hdf5\n",
            "13/13 [==============================] - 194s 15s/step - loss: 2.9147\n",
            "Epoch 13/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9175 \n",
            "Epoch 00013: loss did not improve from 2.91475\n",
            "13/13 [==============================] - 193s 15s/step - loss: 2.9175\n",
            "Epoch 14/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9175 \n",
            "Epoch 00014: loss did not improve from 2.91475\n",
            "13/13 [==============================] - 193s 15s/step - loss: 2.9175\n",
            "Epoch 15/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9186 \n",
            "Epoch 00015: loss did not improve from 2.91475\n",
            "13/13 [==============================] - 194s 15s/step - loss: 2.9186\n",
            "Epoch 16/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9208 \n",
            "Epoch 00016: loss did not improve from 2.91475\n",
            "13/13 [==============================] - 194s 15s/step - loss: 2.9208\n",
            "Epoch 17/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9132 \n",
            "Epoch 00017: loss improved from 2.91475 to 2.91323, saving model to model_weights_saved.hdf5\n",
            "13/13 [==============================] - 192s 15s/step - loss: 2.9132\n",
            "Epoch 18/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9161 \n",
            "Epoch 00018: loss did not improve from 2.91323\n",
            "13/13 [==============================] - 193s 15s/step - loss: 2.9161\n",
            "Epoch 19/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9131 \n",
            "Epoch 00019: loss improved from 2.91323 to 2.91308, saving model to model_weights_saved.hdf5\n",
            "13/13 [==============================] - 193s 15s/step - loss: 2.9131\n",
            "Epoch 20/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9175 \n",
            "Epoch 00020: loss did not improve from 2.91308\n",
            "13/13 [==============================] - 194s 15s/step - loss: 2.9175\n",
            "Epoch 21/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9121 \n",
            "Epoch 00021: loss improved from 2.91308 to 2.91207, saving model to model_weights_saved.hdf5\n",
            "13/13 [==============================] - 192s 15s/step - loss: 2.9121\n",
            "Epoch 22/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9188 \n",
            "Epoch 00022: loss did not improve from 2.91207\n",
            "13/13 [==============================] - 193s 15s/step - loss: 2.9188\n",
            "Epoch 23/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9154 \n",
            "Epoch 00023: loss did not improve from 2.91207\n",
            "13/13 [==============================] - 192s 15s/step - loss: 2.9154\n",
            "Epoch 24/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9165 \n",
            "Epoch 00024: loss did not improve from 2.91207\n",
            "13/13 [==============================] - 194s 15s/step - loss: 2.9165\n",
            "Epoch 25/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9133 \n",
            "Epoch 00025: loss did not improve from 2.91207\n",
            "13/13 [==============================] - 192s 15s/step - loss: 2.9133\n",
            "Epoch 26/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9148 \n",
            "Epoch 00026: loss did not improve from 2.91207\n",
            "13/13 [==============================] - 193s 15s/step - loss: 2.9148\n",
            "Epoch 27/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9108 \n",
            "Epoch 00027: loss improved from 2.91207 to 2.91076, saving model to model_weights_saved.hdf5\n",
            "13/13 [==============================] - 195s 15s/step - loss: 2.9108\n",
            "Epoch 28/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9123 \n",
            "Epoch 00028: loss did not improve from 2.91076\n",
            "13/13 [==============================] - 193s 15s/step - loss: 2.9123\n",
            "Epoch 29/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9060 \n",
            "Epoch 00029: loss improved from 2.91076 to 2.90603, saving model to model_weights_saved.hdf5\n",
            "13/13 [==============================] - 194s 15s/step - loss: 2.9060\n",
            "Epoch 30/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9079 \n",
            "Epoch 00030: loss did not improve from 2.90603\n",
            "13/13 [==============================] - 194s 15s/step - loss: 2.9079\n",
            "Epoch 31/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9063 \n",
            "Epoch 00031: loss did not improve from 2.90603\n",
            "13/13 [==============================] - 192s 15s/step - loss: 2.9063\n",
            "Epoch 32/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9011 \n",
            "Epoch 00032: loss improved from 2.90603 to 2.90111, saving model to model_weights_saved.hdf5\n",
            "13/13 [==============================] - 196s 15s/step - loss: 2.9011\n",
            "Epoch 33/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.9008 \n",
            "Epoch 00033: loss improved from 2.90111 to 2.90082, saving model to model_weights_saved.hdf5\n",
            "13/13 [==============================] - 192s 15s/step - loss: 2.9008\n",
            "Epoch 34/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.8990 \n",
            "Epoch 00034: loss improved from 2.90082 to 2.89902, saving model to model_weights_saved.hdf5\n",
            "13/13 [==============================] - 192s 15s/step - loss: 2.8990\n",
            "Epoch 35/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.8872 \n",
            "Epoch 00035: loss improved from 2.89902 to 2.88724, saving model to model_weights_saved.hdf5\n",
            "13/13 [==============================] - 193s 15s/step - loss: 2.8872\n",
            "Epoch 36/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.8831 \n",
            "Epoch 00036: loss improved from 2.88724 to 2.88310, saving model to model_weights_saved.hdf5\n",
            "13/13 [==============================] - 194s 15s/step - loss: 2.8831\n",
            "Epoch 37/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.8746 \n",
            "Epoch 00037: loss improved from 2.88310 to 2.87455, saving model to model_weights_saved.hdf5\n",
            "13/13 [==============================] - 195s 15s/step - loss: 2.8746\n",
            "Epoch 38/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.8603 \n",
            "Epoch 00038: loss improved from 2.87455 to 2.86028, saving model to model_weights_saved.hdf5\n",
            "13/13 [==============================] - 195s 15s/step - loss: 2.8603\n",
            "Epoch 39/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.8501 \n",
            "Epoch 00039: loss improved from 2.86028 to 2.85006, saving model to model_weights_saved.hdf5\n",
            "13/13 [==============================] - 194s 15s/step - loss: 2.8501\n",
            "Epoch 40/40\n",
            "13/13 [==============================] - ETA: 0s - loss: 2.8332 \n",
            "Epoch 00040: loss improved from 2.85006 to 2.83317, saving model to model_weights_saved.hdf5\n",
            "13/13 [==============================] - 193s 15s/step - loss: 2.8332\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9c34035a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOwCCztneScB"
      },
      "source": [
        "filename = \"model_weights_saved.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwJUOztSeYCD"
      },
      "source": [
        "num_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJi1PsaJeYJ6",
        "outputId": "19a3ef1e-d0e0-4f40-866e-a8da3cdf4936"
      },
      "source": [
        " start = numpy.random.randint(0, len(x_data) - 1)\n",
        "pattern = x_data[start]\n",
        "print(\"Random Seed:\")\n",
        "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:\n",
            "\" reat adventure big trip ease transition next stage life give chance reflect going want end travel education seeing world provides education absolutely impossible get school travel teaches economy politics history geography sociology intense hands way class fortunately school travel always taking applications entrance exam required travel challenges getting daily latte place staring screen nine five every day nearly interesting enough even choose  \"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_k_ioLbho22A"
      },
      "source": [
        "\"nine five every day nearly interesting enough even choose work road keep staring screen find new place drink latte depending destination finding coffee foamy milk good place sip could prove sizeable challenge travel full moments joy challenges overcoming challenges gives greatest joys travel shakes things sucks stuck rut everyone knows like big trip perfect solution fly around world stopping places always wanted visit go ahead plan ideal rout \"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLlCA-9BpIOk"
      },
      "source": [
        "\" willing take first step start planning itinerary waiting put together specials inspire travel gives cool stories let face even folks tell story words last year mongolia get instant party points even events seem trivial nostalgia distance create irresistible spin makes mundane things like getting laundry done zanzibar entertaining person overdo constantly surprised flavors world offer way people cultures countries prepare food break bread togeth \"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odMD3xTbpGqI"
      },
      "source": [
        "\"adventure big trip ease transition next stage life give chance reflect going want end travel education seeing world provides education absolutely impossible get school travel teaches economy politics history geography sociology intense hands way class fortunately school travel always taking applications entrance exam required travel challenges getting daily latte place staring screen nine five every day nearly interesting enough even choose  \"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8RU4C7cpRkk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}